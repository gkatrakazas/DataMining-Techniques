{"cells":[{"cell_type":"markdown","metadata":{"id":"mBYEDKKnsKxH"},"source":["First, we need to import the necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1676125289756,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"9R74iHhGsJ-1","outputId":"d5b52416-201b-4a30-ab44-b05fbfef8eb0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from google.colab import drive \n","import pandas as pd\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"]},{"cell_type":"markdown","metadata":{"id":"h-5y8lzLt2kU"},"source":["Connect to Google Drive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30337,"status":"ok","timestamp":1676125320091,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"TD0UkqtPNVfp","outputId":"d669af74-6765-49b2-cf9a-de8769f7b8c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eBOajtXQuZOX"},"source":["Read Train and Test CSV files"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":24984,"status":"ok","timestamp":1676125345070,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"pemYkUxvXkdG"},"outputs":[],"source":["train=pd.read_csv('/content/drive/My Drive/DataAnalytics/Datasets/Dataset_1/train.csv')\n","test=pd.read_csv('/content/drive/My Drive/DataAnalytics/Datasets/Dataset_1/test.csv')"]},{"cell_type":"markdown","metadata":{"id":"2gJukkWJsBQU"},"source":["Concat title and Content"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":642,"status":"ok","timestamp":1676125345700,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"EmIJ1OTdIZZx"},"outputs":[],"source":["train['Text'] = train['Title'] + ' ' + train['Content']\n","test['Text'] = test['Title'] + ' ' + test['Content']"]},{"cell_type":"markdown","metadata":{"id":"06zbyC_Xeoxh"},"source":["Cleanning"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":245109,"status":"ok","timestamp":1676125590806,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"_CDxd6z6erXQ"},"outputs":[],"source":["def clean_text(text):\n","  # remove punctuation\n","  text = re.sub(r'[^\\w\\s]', '', text)\n","  # lowercase the text\n","  text = text.lower()\n","\n","  # remove stop words\n","  stop_words = set(nltk.corpus.stopwords.words('english'))\n","  tokens = nltk.word_tokenize(text)\n","  text = [token for token in tokens if token not in stop_words]\n","  \n","  # join the tokens into a single string\n","  text = ' '.join(text)\n","  return text\n","\n","train['Text'] = train['Text'].apply(clean_text)\n","test['Text'] = test['Text'].apply(clean_text)"]},{"cell_type":"markdown","metadata":{"id":"gZhgRiSzufYW"},"source":["Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_uPJuJ8v09Z"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train['Text'])\n","X_test = vectorizer.transform(test['Text'])\n","\n","# Split the train data into features (X) and target (y)\n","y_train = train['Label']\n","\n","# Train the Naive Bayes classifier\n","nb = MultinomialNB()\n","nb.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","test_predictions = nb.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","df = pd.DataFrame({'Id': test['Id'], 'Predicted': test_predictions})\n","df.to_csv('/content/drive/My Drive/DataAnalytics/Output/Output_1/results_naive_bayes.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"kmJXuE4tuu_G"},"source":["SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsN-V6vdoaLS"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","\n","# Vectorize the texts using Tfidf\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train['Text'])\n","X_test = vectorizer.transform(test['Text'])\n","\n","# Split the train data into features (X) and target (y)\n","y_train = train['Label']\n","\n","# Train the SVM classifier\n","svm = LinearSVC()\n","svm.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","test_predictions = svm.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","df = pd.DataFrame({'Id': test['Id'], 'Predicted': test_predictions})\n","df.to_csv('/content/drive/My Drive/DataAnalytics/Output/Output_1/results_svm.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"OMHgVdEFQjDe"},"source":["Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2NxEFluynze"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Vectorize the texts using Tfidf\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train['Text'])\n","X_test = vectorizer.transform(test['Text'])\n","# Define the model\n","model = DecisionTreeClassifier()\n","\n","# Fit the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","test_predictions = model.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","df = pd.DataFrame({'Id': test['Id'], 'Predicted': test_predictions})\n","df.to_csv('/content/drive/My Drive/DataAnalytics/Output/Output_1/results_tree.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7300297,"status":"ok","timestamp":1676109194767,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"QInlnxdaIFlM","outputId":"ed989e04-46ec-4f9e-fb5a-eefbb331ac59"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]}],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Vectorize the texts using Tfidf\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train['Text'])\n","X_test = vectorizer.transform(test['Text'])\n","\n","# Split the train data into features (X) and target (y)\n","y_train = train['Label']\n","\n","# Define the model\n","mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=0)\n","\n","# Fit the model on the training data\n","mlp.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","test_predictions = mlp.predict(X_test)\n","\n","# Save the predictions to a CSV file\n","df = pd.DataFrame({'Id': test['Id'], 'Predicted': test_predictions})\n","df.to_csv('/content/drive/My Drive/DataAnalytics/Output/Output_1/results_mlp.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"PpIIfWxcYv22"},"source":["Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":559475,"status":"ok","timestamp":1676109754233,"user":{"displayName":"Grigoris Katrakazas","userId":"07696808186927916016"},"user_tz":-120},"id":"XeZBqOBdt5y2","outputId":"27bee607-dda7-4826-d2cf-e8e39395f95c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","3145/3145 [==============================] - 77s 24ms/step - loss: 0.2249 - accuracy: 0.9260 - val_loss: 0.1646 - val_accuracy: 0.9440\n","Epoch 2/5\n","3145/3145 [==============================] - 76s 24ms/step - loss: 0.1260 - accuracy: 0.9591 - val_loss: 0.1541 - val_accuracy: 0.9501\n","Epoch 3/5\n","3145/3145 [==============================] - 78s 25ms/step - loss: 0.0834 - accuracy: 0.9735 - val_loss: 0.1584 - val_accuracy: 0.9521\n","Epoch 4/5\n","3145/3145 [==============================] - 75s 24ms/step - loss: 0.0586 - accuracy: 0.9819 - val_loss: 0.1615 - val_accuracy: 0.9540\n","Epoch 5/5\n","3145/3145 [==============================] - 80s 25ms/step - loss: 0.0446 - accuracy: 0.9868 - val_loss: 0.1810 - val_accuracy: 0.9552\n","1498/1498 [==============================] - 10s 7ms/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","# Convert the text data into numerical features using one-hot encoding\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=3000)\n","tokenizer.fit_on_texts(train['Text'])\n","X_train = tokenizer.texts_to_matrix(train['Text'])\n","X_test = tokenizer.texts_to_matrix(test['Text'])\n","\n","# Encode the target labels\n","le = LabelEncoder()\n","y_train = le.fit_transform(train['Label'])\n","num_classes = len(set(y_train))\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","\n","# Define the model architecture\n","model = keras.Sequential([\n","    keras.layers.Dense(512, activation='relu', input_shape=(3000,)),\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n","\n","# Make predictions on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=1)\n","test_predictions = le.inverse_transform(test_predictions)\n","\n","# Save the predictions to a CSV file\n","df = pd.DataFrame({'Id': test['Id'], 'Predicted': test_predictions})\n","df.to_csv('/content/drive/My Drive/DataAnalytics/Output/Output_1/results_neural_network.csv', index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMAmmaig49pbmeNbsTfv4/M"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}